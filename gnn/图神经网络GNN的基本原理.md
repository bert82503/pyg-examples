
图神经网络GNN的基本原理
======

> [图神经网络（GNN）的基本原理](https://blog.csdn.net/Cyril_KI/article/details/122058881)

# 前言
本文结合一个具体的无向图来对最简单的一种GNN进行推导。
本文第一部分是数据介绍，第二部分为推导过程中需要用的变量的定义，第三部分是GNN的具体推导过程，最后一部分为自己对GNN的一些看法与总结。


# 1. 数据
利用networkx简单生成一个无向图：

每一个节点都有自己的一些特征，比如在社交网络中，每个节点（用户）有性别以及年龄等特征。
5个节点的特征向量依次为：
```
[[2, 3], [4, 7], [3, 7], [4, 5], [5, 5]]
```
同样，6条边的特征向量为：
```
[[1, 3], [4, 1], [1, 5], [5, 3], [5, 6], [5, 4], [4, 3]]
```


# 2. 变量定义
1. 节点特征向量：节点v的特征向量
2. 节点状态向量：节点v的状态向量
3. 边特征向量：边(v, u)的特征向量

**特征向量**实际上也就是**节点或者边的标签**，这个是图本身的属性，一直保持不变。


# 3. GNN算法
GNN算法的完整描述如下：**Forward向前计算状态，Backward向后计算梯度，主函数通过向前和向后迭代调用来最小化损失。**

## 3.1 Forward
早期的GNN都是RecGNN，即循环GNN。这种类型的GNN基于**信息传播机制**： GNN通过**不断交换邻域信息来更新节点状态，直到达到稳定均衡**。

**由更新公式可知，当所有节点的状态都趋于稳定状态时，此时所有节点的状态向量都包含了其邻居节点和相连边的信息。**

这与图嵌入有些类似：**如果是节点嵌入，我们最终得到的是一个节点的向量表示，而这些向量是根据随机游走序列得到的，随机游走序列中又包括了节点的邻居信息， 因此节点的向量表示中包含了连接信息。**

## 3.2 Backward
在**节点嵌入**中，我们最终得到了**每个节点的表征向量**，此时我们就能利用这些向量来进行**聚类、节点分类、链接预测**等等。

GNN中类似，得到这些节点状态向量的最终形式不是我们的目的，我们的**目的是利用这些节点状态向量来做一些实际的应用，比如节点标签预测。**

因此，如果想要预测的话，我们就**需要一个输出函数来对节点状态进行变换，得到我们要想要的东西**：

最容易想到的就是**将节点状态向量经过一个前馈神经网络得到输出**，也就是说gw可以是一个FNN，同样的，fw也可以是一个FNN：

我们利用gw函数对**节点n**收敛后的**状态向量xn**以及其**特征向量ln**进行变换，就能得到我们想要的输出，比如某一类别，某一具体的数值等等。

在BP算法中，我们**有了输出后，就能算出损失，然后利用损失反向传播算出梯度，最后再利用梯度下降法对神经网络的参数进行更新。**


# 4. 总结与展望
GNN是用于图数据的深度学习架构，它**将端到端学习与归纳推理相结合**，业界普遍认为其有望解决深度学习无法处理的**因果推理、可解释性**等一系列瓶颈问题，是未来3到5年的重点方向。

因此，不仅仅是GNN，图领域的相关研究都是比较有前景的，这方面的应用也十分广泛，比如推荐系统、计算机视觉、物理/化学（生命科学）、药物发现等等。

